{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d21da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4115682-29b8-4c0f-9cb8-a02696e76455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2161384d-4403-40d0-88bf-1b6e687244cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from accelerate import Accelerator\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "def str_to_bool(value):\n",
    "    if value.lower() in ('true', '1'):\n",
    "        return True\n",
    "    elif value.lower() in ('false', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError(f\"Boolean value expected, got {value}\")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d0a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer, k=10,fold_idx=0, mask_prob=0.):\n",
    "        self.IGNORE_INDEX = -100\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        self.PROMPT = '''You are an AI assistant that helps users analyze conversations and solve related problems. Please read the conversation carefully and select the most appropriate answer to the question based on the given options.'''\n",
    "        self.answer_dict = {\n",
    "            \"inference_1\": 0,\n",
    "            \"inference_2\": 1,\n",
    "            \"inference_3\": 2\n",
    "        }\n",
    "\n",
    "        \n",
    "        with open(fname, \"r\", encoding='utf-8') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        fold_size = len(self.data) // k\n",
    "        start = fold_size*fold_idx\n",
    "        end = start + fold_size\n",
    "        self.data = self.data[:start] + self.data[end:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        inp = example[\"input\"]\n",
    "        chat = [\"[Conversation]\"]\n",
    "\n",
    "        for cvt in inp['conversation']:\n",
    "            speaker = cvt['speaker']\n",
    "            utterance = cvt['utterance']\n",
    "            if random.random() < self.mask_prob:\n",
    "                utterance = \"[MASK]\"\n",
    "            chat.append(f\"화자{speaker}: {utterance}\")\n",
    "        chat = \"\\n\".join(chat)\n",
    "\n",
    "        question = f\"[Question]\\n위 대화의 {inp['category']}\"\n",
    "        if (ord(inp['category'][-1]) - ord(\"가\")) % 28 > 0:\n",
    "            question += \"으로\"\n",
    "        else:\n",
    "            question = \"로\"\n",
    "        question += \" 올바른 지문은?\"\n",
    "                \n",
    "        chat += \"\\n\\n\" + question + \"\\n\\n[Option]\\n\"\n",
    "\n",
    "        inferences = [\n",
    "            inp['inference_1'],\n",
    "            inp['inference_2'],\n",
    "            inp['inference_3']\n",
    "        ]\n",
    "        label = self.answer_dict[example[\"output\"]]\n",
    "\n",
    "        order = list(range(len(inferences)))\n",
    "        random.shuffle(order)\n",
    "        \n",
    "        shuffled_inferences = [inferences[i] for i in order]\n",
    "        new_label = order.index(label)\n",
    "        \n",
    "        chat += f\"A. {shuffled_inferences[0]}\\n\"\n",
    "        chat += f\"B. {shuffled_inferences[1]}\\n\"\n",
    "        chat += f\"C. {shuffled_inferences[2]}\"\n",
    "\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": self.PROMPT},\n",
    "            {\"role\": \"user\", \"content\": chat},\n",
    "        ]\n",
    "\n",
    "        source = self.tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        target = f\"{['A', 'B', 'C'][new_label]}. {shuffled_inferences[new_label]}{self.tokenizer.eos_token}\"\n",
    "\n",
    "        target = self.tokenizer(target,\n",
    "                                return_attention_mask=False,\n",
    "                                add_special_tokens=False,\n",
    "                                return_tensors=\"pt\")\n",
    "        target[\"input_ids\"] = target[\"input_ids\"].type(torch.int64)\n",
    "\n",
    "        input_ids = torch.concat((source[0], target[\"input_ids\"][0]))\n",
    "        labels = torch.concat((torch.LongTensor([self.IGNORE_INDEX] * source[0].shape[0]), target[\"input_ids\"][0]))\n",
    "        \n",
    "        return {\n",
    "        'input_ids': input_ids,\n",
    "        \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae0367e-f3df-4f7c-9fcb-a627cb4c2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DevDataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer,k=10, fold_idx=0):\n",
    "        IGNORE_INDEX=-100\n",
    "        self.inp = []\n",
    "        self.trg = []\n",
    "        self.label = []\n",
    "\n",
    "        PROMPT = '''You are an AI assistant that helps users analyze conversations and solve related problems. Please read the conversation carefully and select the most appropriate answer to the question based on the given options.'''\n",
    "        answer_dict = {\n",
    "            \"\": None,\n",
    "            \"inference_1\": 0,\n",
    "            \"inference_2\": 1,\n",
    "            \"inference_3\": 2\n",
    "        }\n",
    "\n",
    "        with open(fname, \"r\", encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "\n",
    "        fold_size = len(data) // k\n",
    "        start = fold_size*fold_idx\n",
    "        end = start + fold_size\n",
    "        data = data[start:end]\n",
    "\n",
    "        \n",
    "        def make_chat(inp):\n",
    "            chat = [\"[Conversation]\"]\n",
    "            for cvt in inp['conversation']:\n",
    "                speaker = cvt['speaker']\n",
    "                utterance = cvt['utterance']\n",
    "                chat.append(f\"화자{speaker}: {utterance}\")\n",
    "            chat = \"\\n\".join(chat)\n",
    "\n",
    "            question = f\"[Question]\\n위 대화의 {inp['category']}\"\n",
    "            if (ord(inp['category'][-1]) - ord(\"가\")) % 28 > 0:\n",
    "                question += \"으로\"\n",
    "            else:\n",
    "                question = \"로\"\n",
    "            question += \" 올바른 지문은?\"\n",
    "                \n",
    "            chat = chat + \"\\n\\n\" + question + \"\\n\\n[Option]\\n\"\n",
    "            chat += f\"A. {inp['inference_1']}\\n\"\n",
    "            chat += f\"B. {inp['inference_2']}\\n\"\n",
    "            chat += f\"C. {inp['inference_3']}\"\n",
    "\n",
    "            return chat\n",
    "        \n",
    "        for example in data:\n",
    "            chat = make_chat(example[\"input\"])\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": PROMPT},\n",
    "                {\"role\": \"user\", \"content\": chat},\n",
    "            ]\n",
    "     \n",
    "            source = tokenizer.apply_chat_template(\n",
    "                message,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            target = \"\"\n",
    "            if example[\"output\"] == \"inference_1\":\n",
    "                target = f\"A. {example['input']['inference_1']}{tokenizer.eos_token}\"\n",
    "            elif example[\"output\"] == \"inference_2\":\n",
    "                target = f\"B. {example['input']['inference_2']}{tokenizer.eos_token}\"\n",
    "            elif example[\"output\"] == \"inference_3\":\n",
    "                target = f\"C. {example['input']['inference_3']}{tokenizer.eos_token}\"\n",
    "                \n",
    "            target = tokenizer(target,\n",
    "                      return_attention_mask=False,\n",
    "                      add_special_tokens=False,\n",
    "                      return_tensors=\"pt\")\n",
    "            target[\"input_ids\"] = target[\"input_ids\"].type(torch.int64)\n",
    "\n",
    "            input_ids = torch.concat((source[0], target[\"input_ids\"][0]))\n",
    "            labels = torch.concat((torch.LongTensor([IGNORE_INDEX] * source[0].shape[0]), target[\"input_ids\"][0]))\n",
    "            self.inp.append(input_ids)\n",
    "            self.label.append(labels)\n",
    "            #self.trg.append(answer_dict[example[\"output\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.label[idx]\n",
    "\n",
    "\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859b6424-a3ac-4320-a0cf-4cc075abb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig,TaskType\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# fmt: off\n",
    "parser = argparse.ArgumentParser(prog=\"train\", description=\"Training about Conversational Context Inference.\")\n",
    "\n",
    "g = parser.add_argument_group(\"Common Parameter\")\n",
    "g.add_argument(\"--model_id\", default='kihoonlee/STOCK_SOLAR-10.7B',type=str,  help=\"model file path\")\n",
    "g.add_argument(\"--tokenizer\", default='kihoonlee/STOCK_SOLAR-10.7B',type=str, help=\"huggingface tokenizer path\")\n",
    "g.add_argument(\"--save_dir\", type=str, default=\"fold6\", help=\"model save path\")\n",
    "g.add_argument(\"--batch_size\", type=int, default=1, help=\"batch size (both train and eval)\")\n",
    "g.add_argument(\"--gradient_accumulation_steps\", type=int, default=4, help=\"gradient accumulation steps\")\n",
    "g.add_argument(\"--warmup_steps\", default=204,type=int, help=\"scheduler warmup steps\")\n",
    "g.add_argument(\"--lr\", type=float, default=5e-5, help=\"learning rate\")\n",
    "g.add_argument(\"--epoch\", type=int, default=10, help=\"training epoch\")\n",
    "\n",
    "\n",
    "g.add_argument(\"--fold_k\", type=int, default=10, help=\"k-fold\")\n",
    "g.add_argument(\"--fold_num\", type=int, default=6, help=\"fold_idx\")\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model = Llama3ForSFT.from_pretrained(\n",
    "        args.model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        #quantization_config=bnb_config,\n",
    "    )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    #only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    #target_modules=[\"o_proj\", \"q_proj\",\"k_proj\",\"v_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias='none',\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "    \n",
    "    \n",
    "    if args.tokenizer == None:\n",
    "        args.tokenizer = args.model_id\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\\n' + message['content']+'\\n\\n'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\\n' + message['content']+'\\n\\n'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\\n'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\\n' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "\n",
    "    global LABEL_IDS, PAD\n",
    "    LABEL_IDS= [tokenizer(i, add_special_tokens=False)['input_ids'][0] for i in ['A','B','C']]\n",
    "    \n",
    "    train_dataset = TrainDataset(\"merge.json\", tokenizer,args.fold_k, args.fold_num)\n",
    "    valid_dataset = DevDataset(\"merge.json\", tokenizer,args.fold_k, args.fold_num)\n",
    "\n",
    "    valid_dataset = Dataset.from_dict({\n",
    "        'input_ids': valid_dataset.inp,\n",
    "        \"labels\": valid_dataset.label,\n",
    "        })\n",
    "    \n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=args.save_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=args.warmup_steps,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.lr,\n",
    "        weight_decay=0.1,\n",
    "        num_train_epochs=args.epoch,\n",
    "        max_steps=-1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        log_level=\"info\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=False,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "        max_seq_length=2048,\n",
    "        packing=True,\n",
    "        seed=42,\n",
    "        report_to=\"none\",\n",
    "        #optim_args='grokadamw'\n",
    "        \n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Step\tTraining Loss\tValidation Loss\n",
    "# 189\t0.026200\t0.016993\n",
    "# 378\t0.000600\t0.026899\n",
    "# 567\t0.000200\t0.017611\n",
    "# 756\t0.000200\t0.018808\n",
    "# 945\t0.000100\t0.015489\n",
    "# 1134\t0.000100\t0.015717\n",
    "# 1323\t0.000100\t0.019951\n",
    "# 1512\t0.000000\t0.018297\n",
    "# 1701\t0.000000\t0.016658\n",
    "# 1890\t0.000100\t0.016785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70352b95-55e4-44ef-ac53-5bd96065dfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc31de4402284ab2a108d98c23e19f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,155,776 || all params: 10,745,679,872 || trainable%: 0.1317\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 819\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2,040\n",
      "  Number of trainable parameters = 14,155,776\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "c:\\Users\\Gachon\\anaconda3\\envs\\jy\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2040' max='2040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2040/2040 1:04:15, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.289756</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.264662</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.386464</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.230877</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406570</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297781</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.236264</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265313</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292797</td>\n",
       "      <td>0.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320015</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-204\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-204\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-204\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-409\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-409\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-409\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-614\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-614\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-614\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-819\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-819\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-819\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-1023\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-1023\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-1023\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-1228\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-1228\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-1228\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-1433\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-1433\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-1433\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-1638\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-1638\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-1638\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "Saving model checkpoint to fold6\\checkpoint-1842\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-1842\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-1842\\special_tokens_map.json\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "C:\\Users\\Gachon\\AppData\\Local\\Temp\\ipykernel_595780\\3560835871.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to fold6\\checkpoint-2040\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-2040\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-2040\\special_tokens_map.json\n",
      "Saving model checkpoint to fold6\\checkpoint-2040\n",
      "loading configuration file config.json from cache at C:\\Users\\Gachon\\.cache\\huggingface\\hub\\models--kihoonlee--STOCK_SOLAR-10.7B\\snapshots\\3e60d55d5e1c63191de31d629380488b9bb5f5b4\\config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"kihoonlee/STOCK_SOLAR-10.7B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in fold6\\checkpoint-2040\\tokenizer_config.json\n",
      "Special tokens file saved in fold6\\checkpoint-2040\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    exit(main(parser.parse_args([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa87251-9c86-415e-a5ac-a788250d30fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475da6c-2d73-4bba-8e2a-afa49f64225c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1bcab-4f61-4b7c-a940-5bbce917a82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1060a0e-7eaa-4d44-9277-cb63ce11198f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c81110ff-1955-4f6c-b437-3eb41e16a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 189\t0.000000\t0.901135\t0.834437\n",
    "# 378\t0.004200\t0.556197\t0.880795\n",
    "# 567\t0.000000\t0.480005\t0.887417\n",
    "# 756\t0.000100\t0.515926\t0.894040\n",
    "# 945\t0.001500\t0.549613\t0.913907\n",
    "# 1134\t0.000000\t0.530577\t0.920530\n",
    "# 1323\t0.000000\t0.536749\t0.913907\n",
    "# 1512\t0.000000\t0.514958\t0.920530\n",
    "# 1701\t0.000000\t0.511815\t0.927152\n",
    "# 1890\t0.000000\t0.515028\t0.920530"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
