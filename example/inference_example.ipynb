{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae0367e-f3df-4f7c-9fcb-a627cb4c2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer):\n",
    "        IGNORE_INDEX=-100\n",
    "        self.inp = []\n",
    "        self.trg = []\n",
    "        self.label = []\n",
    "\n",
    "        PROMPT = '''You are an AI assistant that helps users analyze conversations and solve related problems. Please read the conversation carefully and select the most appropriate answer to the question based on the given options.'''\n",
    "        answer_dict = {\n",
    "            \"\": None,\n",
    "            \"inference_1\": 0,\n",
    "            \"inference_2\": 1,\n",
    "            \"inference_3\": 2\n",
    "        }\n",
    "\n",
    "        with open(fname, \"r\", encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        def make_chat(inp, idx):\n",
    "            chat = [\"[Conversation]\"]\n",
    "            for cvt in inp['conversation']:\n",
    "                speaker = cvt['speaker']\n",
    "                utterance = cvt['utterance']\n",
    "                str(utterance).replace('name', '화자')\n",
    "                chat.append(f\"화자{speaker}: {utterance}\")\n",
    "            chat = \"\\n\".join(chat)\n",
    "\n",
    "            question = f\"[Question]\\n위 대화의 {inp['category']}\"\n",
    "            if (ord(inp['category'][-1]) - ord(\"가\")) % 28 > 0:\n",
    "                question += \"으로\"\n",
    "            else:\n",
    "                question = \"로\"\n",
    "            question += \" 올바른 지문은?\"\n",
    "                \n",
    "            chat = chat + \"\\n\\n\" + question + \"\\n\\n[Option]\\n\"\n",
    "            chat += f\"A. {inp[f'inference_{idx[0]}']}\\n\"\n",
    "            chat += f\"B. {inp[f'inference_{idx[1]}']}\\n\"\n",
    "            chat += f\"C. {inp[f'inference_{idx[2]}']}\"\n",
    "\n",
    "            return chat\n",
    "        permutations = list(itertools.permutations([1,2,3]))\n",
    "        for idx in permutations:\n",
    "            for example in data:\n",
    "                chat = make_chat(example[\"input\"], idx)\n",
    "                message = [\n",
    "                    {\"role\": \"system\", \"content\": PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": chat},\n",
    "                ]\n",
    "         \n",
    "                source = tokenizer.apply_chat_template(\n",
    "                    message,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "    \n",
    "                target = \"\"\n",
    "                # if example[\"output\"] == f\"inference_{idx[0]}\":\n",
    "                #     target = f\"A. {example['input'][f'inference_{idx[0]}']}{tokenizer.eos_token}\"\n",
    "                # elif example[\"output\"] == f\"inference_{idx[1]}\":\n",
    "                #     target = f\"B. {example['input'][f'inference_{idx[1]}']}{tokenizer.eos_token}\"\n",
    "                # elif example[\"output\"] == f\"inference_{idx[2]}\":\n",
    "                #     target = f\"C. {example['input'][f'inference_{idx[2]}']}{tokenizer.eos_token}\"\n",
    "                    \n",
    "                target = tokenizer(target,\n",
    "                          return_attention_mask=False,\n",
    "                          add_special_tokens=False,\n",
    "                          return_tensors=\"pt\")\n",
    "                target[\"input_ids\"] = target[\"input_ids\"].type(torch.int64)\n",
    "    \n",
    "                input_ids = torch.concat((source[0], target[\"input_ids\"][0]))\n",
    "                labels = torch.concat((torch.LongTensor([IGNORE_INDEX] * source[0].shape[0]), target[\"input_ids\"][0]))\n",
    "                self.inp.append(input_ids)\n",
    "                self.label.append(labels)\n",
    "                self.trg.append(answer_dict[example[\"output\"]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.label[idx]\n",
    "\n",
    "\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b023c89-57d9-4618-98dc-92cf982ca773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250],device=shift_labels.device)\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "#             index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "#             true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "#             true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:,label_tokens_ids]\n",
    "#             loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859b6424-a3ac-4320-a0cf-4cc075abb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# fmt: off\n",
    "number = 9\n",
    "parser = argparse.ArgumentParser(prog=\"test\", description=\"Testing about Conversational Context Inference.\")\n",
    "\n",
    "g = parser.add_argument_group(\"Common Parameter\")\n",
    "g.add_argument(\"--output\", default=f'quan8_auto.json', type=str, required=False, help=\"output filename\")\n",
    "g.add_argument(\"--model_id\", default='kihoonlee/STOCK_SOLAR-10.7B', type=str, required=False, help=\"huggingface model id\")\n",
    "g.add_argument(\"--tokenizer\", type=str, help=\"huggingface tokenizer\")\n",
    "g.add_argument(\"--device\", default='cuda', type=str, required=False, help=\"device to load the model\")\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     args.model_id,\n",
    "    #     torch_dtype=torch.bfloat16,\n",
    "    #     device_map=args.device,\n",
    "    #     use_cache=False,\n",
    "    # )\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        # bnb_4bit_use_double_quant=True,\n",
    "        # bnb_4bit_quant_type=\"nf4\",\n",
    "        # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_id,\n",
    "    use_cache=False,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "    \n",
    "    model = PeftModel.from_pretrained(model,\n",
    "                                  #f'fold{number}/checkpoint-2040',\n",
    "                                      'last_d/checkpoint-1890'\n",
    "                                 )\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    if args.tokenizer == None:\n",
    "        args.tokenizer = args.model_id\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "    tokenizer.padding_side = 'right'\n",
    "    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\\n' + message['content']+'\\n\\n'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\\n' + message['content']+'\\n\\n'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\\n'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\\n' }}{% endif %}{% endfor %}\"\n",
    "\n",
    "    global LABEL_IDS\n",
    "    LABEL_IDS= [tokenizer(i, add_special_tokens=False)['input_ids'][0] for i in ['A','B','C']]\n",
    "    dataset = CustomDataset(\"test.json\", tokenizer)\n",
    "\n",
    "    answer_dict = {\n",
    "        0: \"inference_1\",\n",
    "        1: \"inference_2\",\n",
    "        2: \"inference_3\",\n",
    "    }\n",
    "\n",
    "    with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    answer = []\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm.tqdm(range(len(dataset))):\n",
    "            inp, labels = dataset[idx]\n",
    "            outputs = model(\n",
    "                inp.to('cuda').unsqueeze(0),\n",
    "                labels=labels.to('cuda')\n",
    "            )\n",
    "            logits = outputs.logits[:,-1].flatten()\n",
    "            probs = (\n",
    "                torch.nn.functional.softmax(\n",
    "                    torch.tensor(\n",
    "                        [\n",
    "                            logits[tokenizer.vocab['A']],\n",
    "                            logits[tokenizer.vocab['B']],\n",
    "                            logits[tokenizer.vocab['C']],\n",
    "                        ]\n",
    "                    ),\n",
    "                    dim=0,\n",
    "                )\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .to(torch.float32)\n",
    "                .numpy()\n",
    "            )\n",
    "            answer.append(numpy.argmax(probs))\n",
    "            \n",
    "    answer = np.array(answer)\n",
    "    answer = answer.reshape(6, len(answer)//6)\n",
    "    for i, custom_dict in enumerate(list(itertools.permutations([0,1,2]))):\n",
    "        custom_dict = {value: index for index, value in enumerate(custom_dict)}\n",
    "        print(custom_dict)\n",
    "        answer[i] =  np.array([custom_dict[value] for value in answer[i].tolist()])\n",
    "        \n",
    "    from scipy import stats\n",
    "    mode_values = stats.mode(answer, axis=0).mode\n",
    "    for idx, label in enumerate(mode_values):\n",
    "        result[idx][\"output\"] = answer_dict[label]\n",
    "        print(answer_dict[label])\n",
    "    with open(args.output, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(result, ensure_ascii=False, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70352b95-55e4-44ef-ac53-5bd96065dfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7368fd7b514186996c18431739ad66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/3630 [00:00<?, ?it/s]c:\\Users\\Gachon\\anaconda3\\envs\\jy\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 3630/3630 [32:55<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2}\n",
      "{0: 0, 2: 1, 1: 2}\n",
      "{1: 0, 0: 1, 2: 2}\n",
      "{1: 0, 2: 1, 0: 2}\n",
      "{2: 0, 0: 1, 1: 2}\n",
      "{2: 0, 1: 1, 0: 2}\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_2\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_3\n",
      "inference_2\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n",
      "inference_3\n",
      "inference_1\n",
      "inference_1\n",
      "inference_1\n",
      "inference_3\n",
      "inference_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    exit(main(parser.parse_args([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61eadc9-8201-4d1c-90ca-aef1c5b4db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # 첫 번째 JSON 파일 읽기\n",
    "# with open('train.json', 'r', encoding=\"utf-8\") as file:\n",
    "#     json_data1 = json.load(file)\n",
    "\n",
    "# # 두 번째 JSON 파일 읽기\n",
    "# with open('dev.json', 'r', encoding=\"utf-8\") as file:\n",
    "#     json_data2 = json.load(file)\n",
    "\n",
    "# # 두 JSON 데이터를 병합\n",
    "# merged_data = []\n",
    "# for i in json_data1:\n",
    "#     merged_data.append(i)\n",
    "\n",
    "# print(len(merged_data))\n",
    "\n",
    "# for i in json_data2:\n",
    "#     merged_data.append(i)\n",
    "\n",
    "# print(len(merged_data))\n",
    "\n",
    "# with open('merge.json', \"w\", encoding=\"utf-8\") as f:\n",
    "#      f.write(json.dumps(merged_data, ensure_ascii=False, indent=4))\n",
    "    \n",
    "# print(\"두 JSON 파일이 성공적으로 병합되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8f875-ba1c-4bcc-9301-c5dd93d76c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
